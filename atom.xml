<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title></title>
    <link rel="self" type="application/atom+xml" href="https://ruben.io/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://ruben.io"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2025-08-22T00:00:00+00:00</updated>
    <id>https://ruben.io/atom.xml</id>
    <entry xml:lang="en">
        <title>LLM Exploration, Workstation Cooling, and Knowledge Base Plans</title>
        <published>2025-08-22T00:00:00+00:00</published>
        <updated>2025-08-22T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-22/"/>
        <id>https://ruben.io/journal/2025-08-22/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-22/">&lt;p&gt;This recording, made quite late, reflects my recent deep dive into AI and large language models (LLMs). While it might be a distraction from other tasks, I&#x27;ve learned a great deal about setting up and working with these systems.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;llm-exploration-and-tooling&quot;&gt;LLM Exploration and Tooling&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve been focusing on the practical aspects of LLMs, including quantizationâ€”how to quantize models myself and what it entails. I&#x27;ve also looked into fine-tuning, which I haven&#x27;t done yet, but I see its potential utility in certain scenarios. A significant area of interest has been how models invoke and use tools.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m currently using Open Web UI as an interface to my Ollama instance, keeping all models within Ollama. I&#x27;ve successfully converted some models not natively supported by Ollama, such as a quantized version of GPT-OSS, which I&#x27;m eager to test, especially for coding tasks. I anticipate needing different models for different types of work.&lt;&#x2F;p&gt;
&lt;p&gt;I also developed a tool to integrate Kagi into Open Web UI, which is now working well after resolving some type-hinting issues. I&#x27;ve added a couple of tools: one for general thinking (like Kagi) and another for web browsing. I plan to develop a more robust web browser API tool, allowing the assistant to navigate websites, render pages, interact with HTML, and click on elements. This capability is exciting but also a bit daunting, as it grants the model more control, even though current models aren&#x27;t always perfectly reliable. It could enable more human-like interactions, but security measures like CAPTCHAs will likely be prevalent, though perhaps ineffective against such advanced browser tools.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;workstation-power-and-cooling&quot;&gt;Workstation Power and Cooling&lt;&#x2F;h2&gt;
&lt;p&gt;My computer&#x27;s new fan controller is functioning well. I&#x27;ve observed that under heavy GPU load for a few minutes (e.g., 100% utilization for 2-3 minutes), the GPU temperature can reach 80-85 degrees Celsius. However, it quickly drops back to around 50 degrees within 30 seconds once the load is removed. This rapid cooling is encouraging.&lt;&#x2F;p&gt;
&lt;p&gt;I don&#x27;t expect prolonged CPU loads, but if I start training models, it could become an issue. The airflow has significantly improved. I&#x27;m wondering if the fan control, which is managed by the motherboard, is fully aware of the GPU&#x27;s temperature or if it primarily reacts to CPU temperature. This is something I need to investigate further, as better coordination could optimize cooling.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;knowledge-base-and-rag-system-development&quot;&gt;Knowledge Base and RAG System Development&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve started transferring some of my voice recordings, not just for my website, but also into Open Web UI to build a knowledge-based system, essentially a RAG (Retrieval Augmented Generation) system. It appears to be working, but it currently limits responses to three, which is quite restrictive.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m contemplating building a more sophisticated system. My intention is for it to go beyond just retrieving the first relevant document. When searching, it should identify documents, extract concepts from them, and then use a graph database to find related edges and other pertinent documents, which would then be indexed. This more complex pipeline would allow for deeper and more comprehensive information retrieval. However, this is a significant undertaking, and for now, the current system of recording, transcribing, and storing transcripts is sufficient.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;personal-workflow-for-recordings&quot;&gt;Personal Workflow for Recordings&lt;&#x2F;h2&gt;
&lt;p&gt;My current workflow involves making recordings and then transcribing them. I&#x27;m considering an alternative: directly uploading the recordings to the Open Web UI&#x27;s AI to see if it can extract information and store it in the knowledge base, similar to my manual process. This could streamline the entire process and integrate it more tightly with my LLM tools. It&#x27;s an interesting possibility worth exploring.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Architectural Diagrams, Machine Lifecycle, and Home Lab Setup</title>
        <published>2025-08-20T00:00:00+00:00</published>
        <updated>2025-08-20T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-20/"/>
        <id>https://ruben.io/journal/2025-08-20/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-20/">&lt;p&gt;Today&#x27;s focus involved architectural work for Catacloud, implementing a machine lifecycle system, and setting up my home lab.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;architectural-diagrams-and-documentation&quot;&gt;Architectural Diagrams and Documentation&lt;&#x2F;h2&gt;
&lt;p&gt;I spent time on architectural diagrams for Catacloud, creating some decent documentation for the job scheduling system. There are a few areas that could be improved but is good enough. I&#x27;m content with its current design and have begun the implementation phase. This will take some time, but I aim to finish it within the next couple of days for deployment and testing.&lt;&#x2F;p&gt;
&lt;p&gt;Regarding work, I reviewed and finalized the documentation for the validation engine, making some minor code changes to align with it. I also added basic automatic validation for duplicate identifiers. I started looking into Apache Airflow but was quite drained by midday.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;remote-workstation-and-llm-exploration&quot;&gt;Remote Workstation and LLM Exploration&lt;&#x2F;h2&gt;
&lt;p&gt;My remote workstation setup continues to work well. I was able to power on my machine remotely from the office using Home Assistant and run queries on the LLM. This remote access is highly convenient. The script I developed yesterday for power management is working as intended. Home Assistant keeps a history, showing how long the machine was online. Over the last 24h, it was on for approximately seven to eight hours across different segments. This automated power management is great as it prevents unnecessary electricity consumption if I forget to turn the workstation off.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m impressed with the gpt-oss:20b model; it meets all my needs. I also have the gemma3:27b model, which excels at image recognition. Today, I experimented by giving gpt-oss:20b a command to create a prompt for extracting styles from an image. I then used Gemma 3 with that prompt and several images, and it successfully extracted various stylings. I particularly appreciate how it provides insights into its identification process, often revealing influences that I wouldn&#x27;t have known otherwise. This learning aspect is very cool and useful.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mnt-reform-cleaning-and-future-home-lab-plans&quot;&gt;MNT Reform Cleaning and Future Home Lab Plans&lt;&#x2F;h2&gt;
&lt;p&gt;I cleaned my MNT Reform today. I noticed a small, unmarked key with three dots that I don&#x27;t know the purpose of, but I assume it can be mapped to anything. My daughter joined me in taking off and cleaning all the keys and the board underneath, then putting them back. It was a good practice for both of us, especially for her learning letters.&lt;&#x2F;p&gt;
&lt;p&gt;In the next few days, I plan to set up my Raspberry Pi 5 in my home lab rack to act as a reverse proxy, moving all existing proxies from TrueNAS to it. I will also add a local DNS server to ensure that traffic for my local servers (like my music server) stays within the local network, as currently it might be leaving the network unnecessarily. I might also use the Raspberry Pi as a bastion host. I&#x27;ve been considering this for a while. The Raspberry Pi 5, with its 8GB of RAM, is capable of running various applications, I&#x27;m thinking bookmarks, my personal site the proxy server and dns and maybe some other bits and bobs as they arrive.&lt;&#x2F;p&gt;
&lt;p&gt;Another area I want to explore is adding a headless Steam installation to my desktop environment. This would allow me to stream and play games from other computers, as my MNT Reform (ARM64) isn&#x27;t ideal for gaming.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Workstation Setup, LLM Exploration, and Future Improvements</title>
        <published>2025-08-18T00:00:00+00:00</published>
        <updated>2025-08-18T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-18/"/>
        <id>https://ruben.io/journal/2025-08-18/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-18/">&lt;p&gt;Today was focused on setting up my workstation, exploring large language models (LLMs), and planning future improvements.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;workstation-cleaning-and-hardware-installation&quot;&gt;Workstation Cleaning and Hardware Installation&lt;&#x2F;h2&gt;
&lt;p&gt;I began by thoroughly cleaning my workstation, dismantling everything to remove dust with a vacuum, brush, and damp cloth. I installed four Noctua NF-A12 industrial fans (two in the front, two in the back), which are affordable and offer decent performance. I connected a new 1000W power supply and cleaned the CPU block as much as possible. While some dust reappeared on the graphics card after startup, the CPU temperature is holding well.&lt;&#x2F;p&gt;
&lt;p&gt;I also installed the new RX 7900 XTX graphics card. It&#x27;s a large card, fitting just barely. The fans point downwards, almost touching the power supply, which might restrict airflow. For my use case of intensive bursts (like LLM tasks), it seems to manage temperatures adequately, though it heats up quickly.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;software-setup-and-llm-exploration&quot;&gt;Software Setup and LLM Exploration&lt;&#x2F;h2&gt;
&lt;p&gt;I installed a new Debian OS without disk encryption to enable Wake-on-LAN. I then set up two Docker images: Open Web UI, an interface for LLM systems, and Ollama, with AMD graphics card support. Ollama works very fast with the RX 7900 XTX. I&#x27;ve been successfully running models like Deepseek-r1:8b and gpt-oss:20b, finding them quite performant.&lt;&#x2F;p&gt;
&lt;p&gt;I attempted to integrate Ollama with my Neovim Avante plugin, but it didn&#x27;t work well. Avante&#x27;s prompts seem to confuse these local models, though it functions perfectly with official GPT and Gemini models. This is a minor issue for now, as I have other priorities.&lt;&#x2F;p&gt;
&lt;p&gt;For general querying, replacing my previous Kagi usage, this local setup works fantastically. I need to configure Open Web UI to be more agentic, allowing it to integrate search tools properly. Currently, it constructs and executes searches, then summarizes results, which isn&#x27;t ideal. I prefer a system that performs searches only when it deems necessary, potentially conducting multiple searches for better information gathering.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m also considering the Open Web UI project&#x27;s commercial aspects and custom models. I&#x27;m not entirely convinced by its approach, and I might need something more akin to Avante, though I don&#x27;t have time to develop my own solution.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;power-management-and-future-improvements&quot;&gt;Power Management and Future Improvements&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve implemented a script that turns off the workstation after 30 minutes of inactivity, specifically monitoring traffic on SSH (port 22) and Ollama (port 11434). I&#x27;ve also added a Wake-on-LAN button on my Home Assistant for easy remote power-on.&lt;&#x2F;p&gt;
&lt;p&gt;Future improvements include:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fan Control:&lt;&#x2F;strong&gt; Only one of my four new fans is currently connected. I need a control board to connect all fans to the motherboard, allowing them to be controlled based on temperature sensors. This should improve internal airflow.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;GPU Airflow:&lt;&#x2F;strong&gt; I might add a support peg to slightly lift the graphics card, creating more space for airflow underneath and aiding heat dissipation.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Dust Filtration:&lt;&#x2F;strong&gt; I received a PVC mesh that I plan to add for dust capture, particularly on the top grid, to prevent dust ingress. Proper fan configuration should also help maintain positive airflow and reduce dust.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;CPU Cooling Block:&lt;&#x2F;strong&gt; While the current CPU cooling is adequate, I might consider changing the dissipation block in the future if performance degrades.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Finally, I&#x27;m reconsidering posting all these personal journal entries publicly. I think it might be better to store them locally or on my NAS, perhaps using a Zettelkasten-like system. I can always collect these entries later for blog posts, but daily publishing might not be necessary for personal reflections.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Architectural Diagrams, PC Cleaning, and Hardware Upgrades</title>
        <published>2025-08-17T00:00:00+00:00</published>
        <updated>2025-08-17T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-17/"/>
        <id>https://ruben.io/journal/2025-08-17/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-17/">&lt;p&gt;Today was a challenging day for focus, largely due to feeling quite drained and tired. I spent most of my time trying to make sense of complex architectural diagrams for Catacloud and the job procesing pipelines. The sheer number of moving parts made it difficult to concentrate. My approach has been to break down the work into smaller, independent implementation pieces, which allows me to think about them in isolation while maintaining a high-level overview. This compartmentalization, defined by specific events and their business logic, helps manage the complexity, especially in my current mental state.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;pc-maintenance-and-hardware-upgrades&quot;&gt;PC Maintenance and Hardware Upgrades&lt;&#x2F;h2&gt;
&lt;p&gt;I also dedicated time to my desktop computer, which was long overdue for a thorough cleaning. It was astonishingly full of dust. The stock AMD cooling block on the CPU was so caked with dust that you couldn&#x27;t even see the radiator fins underneath. This likely stemmed from not having proper dust filters and only a single exhaust fan, causing air to be pulled in from the top ventilation grid, which has no dust filter. I hope to clean the cooling block properly, but if it&#x27;s beyond salvaging, I might need to replace it or even buy a new one.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m also dealing with the challenge of fitting a much larger graphics card (AMD RX 7900 XTX) into my mITX board case. I received a new 1000-watt power supply today to replace my 750-watt unit, as the new graphics card is very power-hungry. I&#x27;m considering selling my older graphics card and power supply before they lose more value. The cables also need a thorough cleaning as they are covered in dust.&lt;&#x2F;p&gt;
&lt;p&gt;My current CPU, an AMD Ryzen with 8 cores and 16 threads, paired with 32 GB of DDR4 RAM at 3600 MHz, has been sufficient for my needs. For now, this setup will remain. In the future, I might need to upgrade the motherboard, which could lead to a cascading upgrade of the CPU and memory sticks. I&#x27;m not sure if my current CPU socket is compatible with Threadripper, but I could potentially upgrade the motherboard and memory while keeping the same CPU for a while, perhaps with an external water-cooling system if needed.&lt;&#x2F;p&gt;
&lt;p&gt;The new graphics card I acquired seems to have some known issues, particularly with cooling and power efficiency. I&#x27;m hopeful these won&#x27;t lead to frequent crashes. My primary use for this system won&#x27;t involve long gaming sessions, but rather intensive bursts of usage, so we&#x27;ll see how it performs. I really hope I don&#x27;t have to purchase any more components for this setup.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>VPN Script, Recording Tool, and Terminal Theme Improvements</title>
        <published>2025-08-16T00:00:00+00:00</published>
        <updated>2025-08-16T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-16/"/>
        <id>https://ruben.io/journal/2025-08-16/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-16/">&lt;p&gt;Today was not a particularly productive work day, primarily due to a rough night&#x27;s sleep which made it difficult to focus. Despite this, I managed to complete a few simple but useful tasks.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;dotfile-enhancements&quot;&gt;Dotfile Enhancements&lt;&#x2F;h2&gt;
&lt;p&gt;I added a new script to my dotfiles that provides a visual indicator when I&#x27;m connected to a VPN. This helps me quickly confirm my connection status.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;enhanced-recording-script&quot;&gt;Enhanced Recording Script&lt;&#x2F;h3&gt;
&lt;p&gt;This new recording script automates the process of recording audio and then transcribing it. It saves the output directly in the terminal&#x27;s current directory. A key feature of this script is its ability to capture all system audio, including both microphone input and any other audio playing on the system. This is particularly useful for recording meetings, as it ensures all audio is captured. While my current transcription model is designed for a single speaker, I am considering integrating a model that can differentiate between multiple speakers in the future. This would be incredibly valuable for accurately tracking meeting discussions, allowing me to search through transcripts later.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ve observed that while initial transcripts might not be perfectly accurate, their quality improves significantly when provided with context. For example, if you&#x27;re maintaining meeting records on a consistent theme, using an LLM to correct the transcript based on prior context can greatly improve its accuracy in identifying correct words. This approach requires some initial manual curation for the first few entries, but subsequent curations become much easier as the assistant learns to identify and correct common errors, a method I&#x27;ve found effective in my personal journaling.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;terminal-theme-improvements&quot;&gt;Terminal Theme Improvements&lt;&#x2F;h2&gt;
&lt;p&gt;I also dedicated some time to improving my terminal themes. I made minor adjustments to the dark theme for my editor (Neovim), but my primary focus was on adapting the colors of my terminal theme to adhere to standard conventions for terminal colors. This has resulted in much better contrast, making text and elements significantly easier to read. I&#x27;ve noticed that some tools may still output colors that fall outside the 16-color palette of my current theme, but this is a minor issue for now.&lt;&#x2F;p&gt;
&lt;p&gt;That concludes today&#x27;s updates. Looking forward to more progress tomorrow.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Key Management, Hardware Upgrades, and Remote Access Developments</title>
        <published>2025-08-15T00:00:00+00:00</published>
        <updated>2025-08-15T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-15/"/>
        <id>https://ruben.io/journal/2025-08-15/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-15/">&lt;p&gt;Today was primarily focused on enhancing my security setup, acquiring new hardware for a remote workstation, and improving file access.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;key-management-overhaul&quot;&gt;Key Management Overhaul&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve significantly upgraded my key management system. I generated a new offline PGP key that never expires. This master key is used to generate subkeys for authentication, signing, and encryption. I now have two physical YubiKeys:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tiny YubiKey:&lt;&#x2F;strong&gt; This one fits discreetly into a USB port and is currently attached to my MNT Reform. It holds three new subkeys generated from the offline master key.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Keychain YubiKey:&lt;&#x2F;strong&gt; This is my older YubiKey, which I carry on my keychain and has NFC capabilities for use with my phone. It currently holds the old subkeys. My intention is to gradually phase out these old keys, though I could use my backup key to manage them.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I also set up a similar key system for my partner, creating a new PGP key for her. This involves one persistent backup key stored safely offline and subkeys loaded onto an NFC-enabled YubiKey for her authentication needs.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;password-management&quot;&gt;Password Management&lt;&#x2F;h2&gt;
&lt;p&gt;On the topic of password management, I created a shared vault with my partner using GNU &lt;code&gt;pass&lt;&#x2F;code&gt;. The vault is structured with three folders:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;My Folder:&lt;&#x2F;strong&gt; Only my PGP key can decrypt passwords stored here.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Shared Folder:&lt;&#x2F;strong&gt; Both my partner&#x27;s key and mine can decrypt passwords in this folder.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Partner&#x27;s Folder:&lt;&#x2F;strong&gt; Only my partner&#x27;s key can decrypt passwords here.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;This system provides a simple, self-hosted solution for secure password sharing. I plan to document this process thoroughly, including details on extending subkey expiry dates with the main offline key and the storage location of the offline keys, so my partner can manage it independently.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;hardware-upgrade-amd-rx-7900-xtx-remote-workstation&quot;&gt;Hardware Upgrade: AMD RX 7900 XTX &amp;amp; Remote Workstation&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve invested in new hardware: an AMD RX 7900 XTX graphics card. This card, a top-tier consumer model from a few years ago, boasts 24 GB of VRAM. My primary reason for this purchase is to leverage it for machine learning tasks, utilizing AMD&#x27;s ROCm technology, which is comparable to Nvidia&#x27;s CUDA but at a significantly lower cost. I bought it second-hand, hoping it fits into my current PC case.&lt;&#x2F;p&gt;
&lt;p&gt;To accommodate the new GPU, I&#x27;m upgrading my system&#x27;s power supply and adding new fans with dust filters to improve airflow and reduce dust accumulation. My goal is to transform this desktop into a dedicated workload machine and remote workstation. It will remain off until needed for heavy tasks like compiling, running jobs, or potentially hosting self-contained Llama-based models.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m exploring ways to manage its power consumption, ideally powering it on via Wake-on-LAN and having it automatically shut down after a period of inactivity (e.g., 30-60 minutes). This might require developing a custom software solution for power management and remote access.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;nas-and-remote-file-access&quot;&gt;NAS and Remote File Access&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve set up a new volume on my TrueNAS for general files, including documentation. Currently, I can mount this file system on my computer and transfer files directly. However, I still need a secure and efficient way to access these files remotely from outside my network.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m considering two options for a bastion host:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Raspberry Pi:&lt;&#x2F;strong&gt; A small Raspberry Pi (perhaps a Pi Zero 2) could serve as a low-power SSH gateway for network access and file transfers via SFTP. The main concern is its limited memory potentially leading to slow transfers.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;TrueNAS VM:&lt;&#x2F;strong&gt; Running a virtual machine (e.g., with 1GB RAM) directly on the TrueNAS would likely offer better transfer speeds due to direct access to the hard drives. I need to research the best operating system for this purpose.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;My goal is to restore my previous system of synchronizing markdown files (like my todo lists and documentation) to my phone, ensuring I can access and update them easily.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>MNT Reform Kernel Compilation, Local AI Inference, and Linux Ecosystem Exploration</title>
        <published>2025-08-14T00:00:00+00:00</published>
        <updated>2025-08-14T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-14/"/>
        <id>https://ruben.io/journal/2025-08-14/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-14/">&lt;p&gt;This entry provides an update on today&#x27;s efforts, primarily focused on recompiling my MNT Reform kernel and exploring local AI inference solutions.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mnt-reform-kernel-and-npu&quot;&gt;MNT Reform Kernel and NPU&lt;&#x2F;h2&gt;
&lt;p&gt;I spent a significant amount of time attempting to recompile my MNT Reform kernel to enable the Rockchip rknpu kernel module, which would allow me to access the Neural Processing Unit (NPU). Initially, I tried compiling the kernel and patching it with the official Rockchip driver, but I soon realized that this driver relies on a proprietary SDK that isn&#x27;t publicly available, making compilation impossible.&lt;&#x2F;p&gt;
&lt;p&gt;Further investigation revealed that the mainline Linux kernel now directly supports the Rockchip NPU. However, the MNT Reform&#x27;s current kernel does not have this module enabled. My next step is to recompile the kernel with the module enabled.&lt;&#x2F;p&gt;
&lt;p&gt;The challenge lies in installing a custom kernel on the MNT Reform. Unlike systems with traditional BIOS, the MNT Reform uses U-Boot, which I&#x27;m not familiar with. I&#x27;m hesitant to proceed without a clear understanding, as I use this laptop daily and cannot afford to render it unbootable. While the MNT Reform does have a serial port for U-Boot access, requiring a connection from another computer, it&#x27;s a recovery method I&#x27;d prefer to avoid if possible. I&#x27;m confident I&#x27;ll figure it out eventually, but the risk of breaking my primary machine is a significant concern.&lt;&#x2F;p&gt;
&lt;p&gt;Furthermore, I&#x27;m questioning the immediate benefit of having the NPU fully functional.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;local-ai-inference-workstation&quot;&gt;Local AI Inference Workstation&lt;&#x2F;h2&gt;
&lt;p&gt;My exploration into the NPU brought me to the realization that I might achieve more practical results by leveraging a more powerful local environment. I&#x27;m now considering setting up my desktop computer as a dedicated local inference and workstation for running capable AI models.&lt;&#x2F;p&gt;
&lt;p&gt;This workstation could also be used for point cloud analysis. A key challenge will be managing its power consumption, as I won&#x27;t be using it constantly. I need to devise a system to turn it on and off on demand. My current idea is to implement a mechanism where it automatically shuts down after a period of inactivity (e.g., 30 minutes to an hour). Powering it on could potentially be done via Wake-on-LAN, and a system service could monitor activity (e.g., CPU or GPU load) to trigger automatic shutdown when idling. I might be able to integrate this with existing hibernation systems.&lt;&#x2F;p&gt;
&lt;p&gt;This approach is particularly appealing given the high cost of cloud-based AI solutions. While a local solution might require fine-tuning and living with certain limitations, it offers significant cost savings.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;linux-ecosystem-and-learning&quot;&gt;Linux Ecosystem and Learning&lt;&#x2F;h2&gt;
&lt;p&gt;This whole process has highlighted how much more I have to learn about the Linux ecosystem. I&#x27;m contemplating revisiting &quot;Linux From Scratch&quot; to gain a deeper understanding of building a Linux system from bare metal. This would provide valuable insights into how various system components interact. There&#x27;s a vast amount to learn, and time is always a constraint.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>GFF3 Validator Refactor, NPU Exploration, and Academic Reflections</title>
        <published>2025-08-13T00:00:00+00:00</published>
        <updated>2025-08-13T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-13/"/>
        <id>https://ruben.io/journal/2025-08-13/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-13/">&lt;p&gt;Today was less productive than hoped from a shipping perspective, primarily due to a significant refactor of the GFF3 validators. I&#x27;m now separating semantic from syntactic validations.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;gff3-validator-refactor&quot;&gt;GFF3 Validator Refactor&lt;&#x2F;h2&gt;
&lt;p&gt;Syntactic validation errors are reported by readers or transformers as they occur, notifying the validation engine which then decides whether to continue or stop the process based on rule sets.&lt;&#x2F;p&gt;
&lt;p&gt;Semantic validations, on the other hand, require multiple inputs and make sense of a larger context, not just a single feature.&lt;&#x2F;p&gt;
&lt;p&gt;The challenges I face at the moment are mainly with semantic validations, that could be doing validations at a feature level, potentially avoiding having to parse a whole annotation before deciding its not correct.&lt;&#x2F;p&gt;
&lt;p&gt;Some code cleanup is also needed as it&#x27;s currently a bit &quot;gnarly.&quot;&lt;&#x2F;p&gt;
&lt;p&gt;The validation engine for EBI work is mostly functional. I need to migrate existing validations to it. A specific challenge involves handling file-level header validations, particularly for GFF3, which might require a new validation type. Integrating the GFF3 tool into the main pipeline is also a next step.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;npu-exploration-on-mnt-reform&quot;&gt;NPU Exploration on MNT Reform&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;m exploring running AI models on my MNT Reform, which has a 3588 processor module with 32GB of RAM and an NPU. My goal is to leverage this hardware to run LLMs locally and offline, especially for documentation and processing journal transcripts. This would reduce reliance on cloud providers like Google and explore self-hosting options. I was also looking at Hetzner, which offers powerful machines for a reasonable monthly cost.&lt;&#x2F;p&gt;
&lt;p&gt;I attempted to run an LLM using a local container, but it&#x27;s very slow and doesn&#x27;t utilize the NPU. To use the NPU, I need to install a Rockchip SDK and its dependencies, including PyTorch. Compiling these dependencies is proving to be a lengthy process. I have compiled the LLM binary itself, but I still need to transform Hugging Face transformer models into a format runnable on the NPU, which requires the slow compilation process. I hope to make progress on this tomorrow.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;academic-and-mathematical-reflections&quot;&gt;Academic and Mathematical Reflections&lt;&#x2F;h2&gt;
&lt;p&gt;A co-worker recommended I start learning mathematics from scratch, suggesting Math Olympiad problems at a middle school level. I found them surprisingly non-trivial. She also recommended set theory and then algebra, topics for which I already own books. I&#x27;ve previously looked into set theory but need to revisit it for better retention.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m considering resuming my mathematics degree with the Open University. However, it&#x27;s expensive, and I&#x27;m valuing my time and avoiding high-pressure environments, especially with personal projects like Catacloud. My long-term aspiration (10-15 years out) is to move into fundamental research and academia, ideally pursuing a PhD, as I enjoy working at the edge of knowledge. While I recognize the limited financial returns, the work itself is the primary driver. I&#x27;m weighing the cost of the Open University against my financial situation and considering cheaper alternatives like the Open University of Catalonia, though I&#x27;m unsure if their degrees align with my interests. I also question whether computation or mathematics is the right path and my aptitude for mathematics.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Tooling Improvements, Catacloud Progress, and Reflections</title>
        <published>2025-08-12T00:00:00+00:00</published>
        <updated>2025-08-12T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-12/"/>
        <id>https://ruben.io/journal/2025-08-12/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-12/">&lt;p&gt;Today was a mix of tooling improvements, work on Catacloud, and some long-term personal reflections.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;editor-and-terminal-theme-enhancements&quot;&gt;Editor and Terminal Theme Enhancements&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve been working on a very simple light theme for my Neovim setup, which seems to be working fine. I also modified my Neovim configurations to dynamically pick either the light or dark theme based on the terminal&#x27;s theme. This is quite useful because my workspace in the mornings has a lot of light and reflections on the screen, making a dark background hard to read. A light background significantly improves readability in such conditions.&lt;&#x2F;p&gt;
&lt;p&gt;My light terminal theme, derived from the Neovim theme, isn&#x27;t perfect and needs some improvements, which I&#x27;ll address at some point. I also spent time restructuring my dot files to be cleaner, so I no longer have to manually link all of them at once. The themes now seem stable, though the terminal themes still require more work.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;recording-system-transition&quot;&gt;Recording System Transition&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;m also testing a new recording system called &quot;Asak,&quot; which is a terminal tool, replacing the GNOME recorder. This tool directly creates a WAV file without external dependencies, saving me processing time as it can be directly used by Whisper.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;job-processing-infrastructure-and-catacloud-progress&quot;&gt;Job Processing Infrastructure and Catacloud Progress&lt;&#x2F;h2&gt;
&lt;p&gt;Yesterday, I refined the job processing infrastructure proposal, which outlines new aggregates to track machine states for commissioning and decommissioning.&lt;&#x2F;p&gt;
&lt;p&gt;For Catacloud, I plan to implement the machine provisioning system. I want to support different backends for provisioning depending on the machine type. This might involve a trait on the machine type itself that implements a &lt;code&gt;provision&lt;&#x2F;code&gt; method. One of the backends will be Hetzner, and I&#x27;ll also add a local development backend for testing, likely using Docker to create new machines. This part is going to be complex.&lt;&#x2F;p&gt;
&lt;p&gt;Once the job processing pipeline and machine provisioning infrastructure are implemented, I&#x27;ll need to integrate some of these events with the billing system. We want to charge based on how long machines are turned on, not just job duration. This will require focusing more on billing again.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m currently stuck on the implementation for Catacloud. Apart from cleaning up designs and reviewing them, I&#x27;ve implemented the aggregates and part of the saga, but it&#x27;s not yet working. I hope to have it functional by the end of the week. I also need to update the supervisor as the job pulling mechanism will change. I&#x27;m considering adding a separate scheduling saga to allocate jobs to machines, distinct from the provisioning saga, to manage the complexity of machine pools and states.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;firefox-extension-and-debugging-challenges&quot;&gt;Firefox Extension and Debugging Challenges&lt;&#x2F;h2&gt;
&lt;p&gt;I added a new Firefox extension to visualize markdown, which makes writing markdown in Neovim much more convenient. While it doesn&#x27;t automatically reload, I can easily refresh the page.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;blog-updates-and-future-plans&quot;&gt;Blog Updates and Future Plans&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve spent a few hours updating the blog, transcribing, cleaning, and publishing journal entries from the last week. I noticed some styles in the blog aren&#x27;t quite right, and I&#x27;d like to use the light style I created for my editor. I might add an option to toggle between dark and light modes, or even recognize the user&#x27;s system theme preference. I&#x27;ll add this to my backlog as I don&#x27;t have time to implement it now.&lt;&#x2F;p&gt;
&lt;p&gt;Speaking of backlogs, I need a better system for appending tasks and tracking them. My current plain.so setup isn&#x27;t ideal, even though I can deploy it locally. I need something as easy as my current recording process, perhaps extracting information from recordings to create or update tickets automatically. It needs to be something that doesn&#x27;t get in the way and can provide reports on pending tasks and priorities.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;academic-aspirations-and-financial-considerations&quot;&gt;Academic Aspirations and Financial Considerations&lt;&#x2F;h2&gt;
&lt;p&gt;Over the last few days, I&#x27;ve been considering continuing my mathematics degree with the Open University. It&#x27;s expensive, and I paused it when my daughter was born. Now that I have a bit more time, I&#x27;d like to go back. However, I&#x27;m valuing my time and want to avoid high-pressure environments, especially with my other projects like Catacloud. This means I can&#x27;t afford high university fees.&lt;&#x2F;p&gt;
&lt;p&gt;My long-term vision (10-15 years from now) is to move into fundamental research and academia, ideally pursuing a PhD. I enjoy working on the edge of knowledge, exploring new things, and advancing knowledge rather than repetitive work. I believe academia is the right environment for me, despite its potential political and cultural issues. I know there isn&#x27;t much money in it, but I&#x27;m primarily driven by the work itself.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m not sure if the Open University&#x27;s cost is justified, especially given my current financial situation. I&#x27;m enjoying working at EBI due to its flexibility, hoping my personal projects will eventually provide more financial independence. I&#x27;ve also considered the Open University of Catalonia, which is cheaper, but I&#x27;m unsure if their degrees align with what I want to study. I might be wrong about needing a university route to publish and be part of a research group, but that&#x27;s my impression from observing academia. I also question whether computation or mathematics is the right path for me. I find beauty in mathematics, but I&#x27;m not sure if I&#x27;m good at it. These are all doubts I&#x27;m currently grappling with.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;ebi-work-validation-engine-and-gff3-integration&quot;&gt;EBI Work: Validation Engine and GFF3 Integration&lt;&#x2F;h2&gt;
&lt;p&gt;Regarding EBI work, the validation engine seems to be working pretty much now. I need to migrate some validations to it. There&#x27;s a question about validating headers, which are file-level validations that come before features. I might add a new specific validation type, perhaps only for GFF3, to handle header validation. This seems workable. I also need to integrate the GFF3 tool into the actual pipeline.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Machine Life Cycle on Catacloud</title>
        <published>2025-08-11T00:00:00+00:00</published>
        <updated>2025-08-11T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-11/"/>
        <id>https://ruben.io/journal/2025-08-11/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-11/">&lt;p&gt;Today&#x27;s focus is on implementing the machine life cycle for Catacloud. This new system will monitor job states, provisioning new machines for jobs, and then de-provisioning the machines once the jobs are complete.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;new-entities-machine-type-and-machine-instance&quot;&gt;New Entities: Machine Type and Machine Instance&lt;&#x2F;h2&gt;
&lt;p&gt;To support this, we are introducing new entities:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Machine Type&lt;&#x2F;strong&gt;: This entity will describe the specifications of a machine. It will serve as a mapping that the machine life cycle uses to identify and provision the correct machine, initially focusing on Hetzner Cloud, but with future potential for other cloud providers, local docker instances or even perpetual baremetal running machines.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Organization and Machine Description (intermediary table)&lt;&#x2F;strong&gt;: This table will link organizations to the machine types they have access to.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Machine Instance&lt;&#x2F;strong&gt;: The machine life cycle will create a new machine instance when jobs are in a pending state. It will track the job&#x27;s progress and then turn off the machine instance when the job finishes.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;job-scheduling-and-machine-types&quot;&gt;Job Scheduling and Machine Types&lt;&#x2F;h2&gt;
&lt;p&gt;Jobs will be scheduled to specific machine types through their job configuration (previously referred to as pipeline configuration or job type). This means that jobs with a particular configuration will be defined to run on a specific machine type. This binding is crucial for scenarios where certain job types might require specific resources, such as GPUs. When defining a new job configuration, users will select from the machine types their organization has access to.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;machine-life-cycle-logic-and-resource-management&quot;&gt;Machine Life Cycle Logic and Resource Management&lt;&#x2F;h2&gt;
&lt;p&gt;The machine life cycle will be bound to an organization&#x27;s machine type configuration. This configuration will define the maximum amount of resources an organization can schedule, such as the maximum number of simultaneous jobs or the maximum pool size of machines. For example, if an organization only wants to run two jobs simultaneously, the maximum pool size will be two machines. If they only have one, jobs will be queued one by one.&lt;&#x2F;p&gt;
&lt;p&gt;I may need to implement multiple algorithms for the machine life cycle to decide job parallelism. The goal will be to optimize resource utilization, potentially by reducing costs (scheduling the fewest machines possible) or by prioritizing speed (creating more machines for faster completion). This will involve considering factors like minimum machine run times (e.g., a 30-minute job on a machine with a one-hour minimum run time).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;architectural-considerations&quot;&gt;Architectural Considerations&lt;&#x2F;h2&gt;
&lt;p&gt;The machine life cycle will be implemented as a saga. The machine type and machine instance will be aggregates.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Permissions, Orchestrators, and Pipeline Editor</title>
        <published>2025-08-09T00:00:00+00:00</published>
        <updated>2025-08-09T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-09/"/>
        <id>https://ruben.io/journal/2025-08-09/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-09/">&lt;p&gt;Today&#x27;s focus was on refining the permission system for Catacloud, exploring solutions for command and read access, and designing orchestrators to handle complex entity operations. I also touched upon ongoing work with the pipeline configuration editor and persistent file uploads.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;permission-system-refinements&quot;&gt;Permission System Refinements&lt;&#x2F;h2&gt;
&lt;p&gt;Currently, Catacloud&#x27;s permission system operates on command handlers and appends filters to queries for projections. This is a traditional policy enforcement and decision point system. Ideally, I&#x27;d like to implement a more comprehensive Attribute-Based Access Control (ABAC) system by extending Epoch with new traits. This would decouple authorization logic from data access, making testing much simpler. While the existing filtering system for queries is robust and has been developed over years, I&#x27;m deferring a full ABAC implementation for now.&lt;&#x2F;p&gt;
&lt;p&gt;For immediate needs, I&#x27;m adding a function to command handlers to perform policy enforcement. This function will extract required attributes from the command, reducing boilerplate code for repetitive checks (e.g., &quot;admin can modify, otherwise only owner&quot;).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;orchestrators-for-complex-operations&quot;&gt;Orchestrators for Complex Operations&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve also been considering orchestrators as a solution for commands that affect multiple entities. For instance, if a command targets one entity but needs to delete a bunch of related ones (like unlinking multiple files from a job), an orchestrator can pick up an event emitted by the initial command and then cascade operations to other entities. This approach helps manage complex dependencies and ensures consistency across related data.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;pipeline-configuration-editor&quot;&gt;Pipeline Configuration Editor&lt;&#x2F;h2&gt;
&lt;p&gt;My current main task is adding a configuration editor for pipelines. I&#x27;ve found a promising JSON editor project with a tree system that integrates well for navigating JSON structures. The challenge now is integrating it with HTMLX (or Alpine.js) to properly send requests to the backend.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;file-upload-challenges&quot;&gt;File Upload Challenges&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;m still encountering issues with file uploads, particularly larger files. The web worker occasionally stops, especially when the profiler isn&#x27;t active. My current thought is to ensure the state is persisted so users can resume uploads if the worker dies. This might involve prompting the user to re-select the file, but it would provide a more robust experience.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Epoch Refinements and Performance Considerations</title>
        <published>2025-08-08T00:00:00+00:00</published>
        <updated>2025-08-08T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-08/"/>
        <id>https://ruben.io/journal/2025-08-08/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-08/">&lt;p&gt;Today, I focused on making improvements to Epoch, particularly concerning how events are persisted and how the aggregate state is managed.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;epoch-improvements&quot;&gt;Epoch Improvements&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve implemented changes in Epoch that primarily affect the order in which the aggregate persists events. Additionally, the aggregate now pre-hydrates its state if it wasn&#x27;t already present. This means we could potentially wipe the entire table, and the aggregate would be capable of reconstructing its state from the event stream before applying any new events.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;performance-considerations&quot;&gt;Performance Considerations&lt;&#x2F;h2&gt;
&lt;p&gt;One aspect that has become painfully obvious is that the performance of this library is not optimal. I&#x27;m currently using &lt;code&gt;clone&lt;&#x2F;code&gt; extensively, and there are many static instances scattered throughout the codebase. These areas will require significant work at some point to improve overall performance.&lt;&#x2F;p&gt;
&lt;p&gt;Despite the performance issues, from a usability perspective, Epoch is starting to become quite useful.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;event-versioning&quot;&gt;Event Versioning&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve been contemplating whether to continue forcing users to manually set the event version. I&#x27;ve decided that it would be more intuitive and robust for the aggregate itself to set the event version, deriving it from the previous version of the state. This approach simplifies the API for users and ensures consistency.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Billing UI, Event Sourcing, and Epoch Refinements</title>
        <published>2025-08-07T00:00:00+00:00</published>
        <updated>2025-08-07T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-07/"/>
        <id>https://ruben.io/journal/2025-08-07/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-07/">&lt;p&gt;Today, my main focus was on the Catacloud platform, specifically developing the billing UI and implementing the billing aggregate.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;billing-system-development&quot;&gt;Billing System Development&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve implemented the billing aggregate, which tracks operational metrics like total storage used by an organization and the amount of gigabyte-hours consumed. The system now has a saga that processes these metrics. I&#x27;ve also developed the UI to display daily aggregations, providing good tracking for gigabytes per hour and execution times.&lt;&#x2F;p&gt;
&lt;p&gt;Looking ahead, I plan to extend this system to support multiple services, each with specific pricing per customer, allowing for volume-based accounts and discounts. This design also accounts for future pricing changes without necessarily affecting existing customers, providing flexibility for potential migrations to new pricing models.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;file-upload-and-event-versioning-issues&quot;&gt;File Upload and Event Versioning Issues&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve started investigating why file uploads are failing, particularly larger files, when they go through the event system. It appears to be a concurrency issue. When multiple file parts are uploaded simultaneously, events are sometimes created with the same version in the database, leading to &quot;duplicate key&quot; errors.&lt;&#x2F;p&gt;
&lt;p&gt;The root cause seems to be the order of persistence: currently, the aggregate state is persisted before the events. This is problematic because if state persistence fails, events might already be in the database, leading to an outdated state when reconstructed. Ideally, events should be persisted first, followed by the aggregate state, preferably within a transaction. This ensures that the state is always up-to-date with the latest events.&lt;&#x2F;p&gt;
&lt;p&gt;To optimize performance, we might need to implement a snapshotting mechanism for the aggregate state, so we don&#x27;t have to re-aggregate all events every time. Caching frequently retrieved states in memory could also significantly improve performance.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;epoch-improvements-for-event-hydration&quot;&gt;Epoch Improvements for Event Hydration&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve begun making changes to Epoch to improve how events are read from the event store. Instead of reading the entire event stream, I want to read only new events since the last known version. This led me down a rabbit hole of modifying Epoch&#x27;s traits to support streaming events directly into the rehydration function.&lt;&#x2F;p&gt;
&lt;p&gt;Working with event streams (e.g., pulling events from Postgres as a stream) would be significantly more efficient, especially for rehydrating new projections from the beginning of time, as it avoids building the entire event store in memory. However, this introduces complexities with error propagation when reading from the stream, as the aggregate needs to handle potential errors during event retrieval.&lt;&#x2F;p&gt;
&lt;p&gt;The ultimate goal is to make all I&#x2F;O asynchronous, allowing for better concurrency and multi-threading, which would be a significant improvement. The question remains whether to continue with these fundamental changes to Epoch now, given that the immediate problem is the file upload concurrency issue. I need to re-evaluate if the file upload issue is indeed a race condition that these changes would help resolve.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;atomic-operations-and-transactions&quot;&gt;Atomic Operations and Transactions&lt;&#x2F;h2&gt;
&lt;p&gt;The core problem boils down to ensuring atomic operations when persisting events and state. If the state persistence fails, the events should not be committed, or the system needs a robust way to recover. While a database transaction would be the ideal solution, I need to explore how to achieve this atomicity within Epoch without necessarily relying on explicit database transactions if they are not readily available or suitable. This requires careful consideration of how to handle state and event consistency.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Event Bus Refinements and Aggregate Principles</title>
        <published>2025-08-06T00:00:00+00:00</published>
        <updated>2025-08-06T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-06/"/>
        <id>https://ruben.io/journal/2025-08-06/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-06/">&lt;p&gt;I made a significant mistake while working on the in-memory event bus. I added queues unnecessarily, even though there aren&#x27;t distinct channels per subscriber. I&#x27;m calling subscribers one by one with the event, so a simple list of subscribers is more efficient.&lt;&#x2F;p&gt;
&lt;p&gt;Initially, I had a mutex locking the entire array of subscribers. This caused timeouts because when an event was emitted (e.g., by a saga), it would lock the subscriber array. If the saga then emitted more events, these would attempt to re-access the locked array, leading to a deadlock. Events would pile up in the channel&#x27;s buffer.&lt;&#x2F;p&gt;
&lt;p&gt;The actual solution was to either remove the mutex entirely (as it wasn&#x27;t truly needed) or, if dynamic subscription&#x2F;unsubscription was required, implement a read-write lock. A read-write lock allows multiple parallel reads but only one write at a time, which solves the problem with the subscriber array. I&#x27;ve implemented a read-write lock for now, as it was easier than completely removing the existing mechanism, even though dynamic subscription isn&#x27;t strictly necessary.&lt;&#x2F;p&gt;
&lt;p&gt;This solution has resolved the immediate problem. In the future, I might remove the subscriber array entirely and use a channel with multiple subscribers reacting in parallel to events, potentially each in its own task.&lt;&#x2F;p&gt;
&lt;p&gt;One crucial realization from this experience is that an aggregate should &lt;em&gt;never&lt;&#x2F;em&gt; depend on a projection. Aggregates should not read state from a projection to hydrate or create new commands, as that state might be outdated, leading to inconsistencies. Ideally, the command sent to the aggregate should contain all necessary information and rely only on the aggregate&#x27;s own state, without internal links to projections.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m considering making command handlers synchronous to enforce this, which would effectively lock the application unless threads or other parallel processing mechanisms are used for IO. Currently, aggregate command handlers are asynchronous because I initially wanted to perform IO to call projections from them, which I now believe is a bad idea. I may event add a semaphore system in the future in epoch so this behavior is enforced.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Epoch Changes, Event Sourcing, and Future Plans</title>
        <published>2025-08-04T00:00:00+00:00</published>
        <updated>2025-08-04T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-04/"/>
        <id>https://ruben.io/journal/2025-08-04/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-04/">&lt;p&gt;I&#x27;ve implemented several changes to Epoch, primarily addressing a locking issue and refining versioning within the event sourcing system.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;resolving-locking-issues&quot;&gt;Resolving Locking Issues&lt;&#x2F;h2&gt;
&lt;p&gt;Previously, iterating through subscribers caused mutex locking, likely due to multiple concurrent access attempts. To mitigate this, I introduced a channel that acts as a buffer. Messages now queue up and are processed in batches of ten, rather than all at once. This change has successfully addressed the locking problem.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;event-sourcing-versioning-challenges&quot;&gt;Event Sourcing Versioning Challenges&lt;&#x2F;h2&gt;
&lt;p&gt;A significant challenge arose with aggregate versions not incrementing correctly. The root cause was my oversight in not hydrating specific events for aggregates, as they didn&#x27;t appear to alter the state. However, this led to versioning conflicts. Epoch, by design, does not automatically increment the aggregate&#x27;s version when persisting a command; instead, it expects the hydration process within the event handler to manage this.&lt;&#x2F;p&gt;
&lt;p&gt;Consequently, if the aggregate&#x27;s version isn&#x27;t incremented in the event handler, subsequent events for the same aggregate are saved with an outdated version, leading to &quot;duplicate key&quot; errors and conflicts. While manually incrementing the version in each event handler resolves this, it feels redundant. I&#x27;m exploring ways to make Epoch less error-prone, perhaps by adding a &lt;code&gt;set_version&lt;&#x2F;code&gt; or &lt;code&gt;increase_version&lt;&#x2F;code&gt; method to the state trait. This would allow Epoch to manage versioning automatically during hydration, similar to how update timestamps could also be handled.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;current-progress-and-next-steps&quot;&gt;Current Progress and Next Steps&lt;&#x2F;h2&gt;
&lt;p&gt;Despite these complexities, the system is now largely functional. The saga for computing storage usage is working well, successfully increasing storage size based on events. However, it&#x27;s not yet handling deletions correctly. This appears to be due to remnants of older logic that aren&#x27;t fully integrated with the new event-sourcing methodology, requiring further cleanup.&lt;&#x2F;p&gt;
&lt;p&gt;Overall, I&#x27;m relieved to have resolved the major impediments. All necessary metrics for building are now operational. My immediate focus shifts to creating dashboards: one for users to visualize their costs and breakdowns, and another for administrators. I also need to implement the machine deployment automation system. I plan to begin with the user dashboard tomorrow, aiming to have all dashboards ready by Wednesday. This will necessitate redeploying the application, as the deployment was previously interrupted. Achieving this will put us in a strong position to meet the end-of-month production deadline, which is rapidly approaching.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Catacloud Refactoring and Event Sourcing Challenges</title>
        <published>2025-08-03T00:00:00+00:00</published>
        <updated>2025-08-03T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-03/"/>
        <id>https://ruben.io/journal/2025-08-03/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-03/">&lt;p&gt;Today, I finalized the refactoring of Catacloud to incorporate the changes from your epoch. I found some additional modifications were needed, so I updated the interface of the PostgreSQL state store to make it more generic. This is something I&#x27;ve discussed in previous journal entries, and was done to avoid binding the aggregate to one specific table and ID. Now, the &lt;code&gt;PGState&lt;&#x2F;code&gt; interface requires &lt;code&gt;getByID&lt;&#x2F;code&gt;, &lt;code&gt;persist&lt;&#x2F;code&gt;, &lt;code&gt;abstract&lt;&#x2F;code&gt;, and &lt;code&gt;delete&lt;&#x2F;code&gt; methods. The underlying state also doesn&#x27;t necessarily have to be a &lt;code&gt;PgRow&lt;&#x2F;code&gt;; it could be anything, only the parts of it would need to be persistable in PostgreSQL, potentially across multiple tables. These methods now accept an &lt;code&gt;executor&lt;&#x2F;code&gt;, which allows us to pass in a transaction once they are implemented. Hopefully, this means we won&#x27;t need to change any state implementations once the transaction implementation is available.&lt;&#x2F;p&gt;
&lt;p&gt;Furthermore, in Catacloud, almost everything now uses event sourcing, except for users. I&#x27;ve added organizations and job configurations as aggregates. I also implemented a saga to compute storage usage.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;event-sourcing-challenges-and-race-conditions&quot;&gt;Event Sourcing Challenges and Race Conditions&lt;&#x2F;h2&gt;
&lt;p&gt;While testing, I encountered an interesting, or rather annoying, error in the logs. This saga, which computes storage usage, listens for events from the files aggregate, specifically for &quot;part uploaded&quot; and &quot;file deleted&quot; events. Based on these events, it sends commands to the organization aggregate to update the organization&#x27;s storage usage.&lt;&#x2F;p&gt;
&lt;p&gt;The issue is a race condition: the handler sends a command, we read the state from the store, apply the events, and then attempt to persist the state. This persistence fails with a &quot;duplicate key&quot; error, or a unique key constraint violation. This indicates that two events with the same version are trying to modify the same thing simultaneously. Since there are no atomic operations to persist the state and events together, this race condition occurs.&lt;&#x2F;p&gt;
&lt;p&gt;The solution is to run a transaction between getting the state for the organization and persisting events. This will ensure the version of the events is correct. While this is fixable, it&#x27;s a bit annoying because I wasn&#x27;t planning on implementing transactions yet, and it&#x27;s a significant amount of work given my limited time. This issue has been happening quite consistently, and it&#x27;s why the storage usage computation is incorrect. The projection that listens for the storage usage event isn&#x27;t being triggered because the events aren&#x27;t being persisted. This is a critical issue that needs to be addressed.&lt;&#x2F;p&gt;
&lt;p&gt;Overall, once I fix this, we are moving in the right direction. However, I must admit this whole system is quite complex now. It&#x27;s entirely event-driven, and we&#x27;re not calling things directly; instead, we&#x27;re sending commands and listening to events everywhere. This makes it a lot looser than when you call a function directly, which I believe is one of the main issues with event-driven systems. I&#x27;m currently using an in-memory event bus. If I were to use an external event bus, some of these issues would be more apparent due to added latency. These are not trivial problems, but I&#x27;m committed to seeing this through.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Catacloud Refactor and Garden Adventures</title>
        <published>2025-08-02T00:00:00+00:00</published>
        <updated>2025-08-02T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-02/"/>
        <id>https://ruben.io/journal/2025-08-02/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-02/">&lt;p&gt;This morning, I focused on Catacloud, specifically on refactoring due to the Epoch changes from yesterday. It&#x27;s now much cleaner. I also added organizations, which required another significant refactor. I haven&#x27;t fully linked everything through the organization yet, and I&#x27;m quite tired, but the organization feature is now implemented in Catacloud. The website is working, though it&#x27;s still using links through the user. Once I remove those, I&#x27;ll have to change the queries to join through the organization relationship. I haven&#x27;t implemented sagas yet, but that&#x27;s what I plan to do tomorrow, along with finishing the organization integration.&lt;&#x2F;p&gt;
&lt;p&gt;Because of the organization changes, I also had to modify all the conventions, and I&#x27;m unsure if it will work with Supabase, our current database provider. We use their authentication tooling. Locally, I have my own login tooling, and I could deploy it with that, but in theory, it should work because I&#x27;m using the same functions as Supabase on the database side. It might not, though.&lt;&#x2F;p&gt;
&lt;p&gt;I also need to create endpoints for creating organizations and add a new login flow for users. Ideally, as admins, we would add an organization, or perhaps allow users to register and create the organization at that point. If users register and create the organization, that user would become the administrator and wouldn&#x27;t be able to do anything until a payment system is integrated. I guess that flow, where users register themselves, should work.&lt;&#x2F;p&gt;
&lt;p&gt;The other option is that we create the organizations ourselves, and then when a user tries to log in, they would add the company&#x27;s domain, for example, and then they could recognize the domain and log in. We&#x27;d have to add something in the organization&#x27;s dashboard to enable that functionality. Another option is that we create the organization and send an invite to the first administrator, who can then invite other users. When they accept the invite, they set up a password and register. I think I need to implement this flow before going to production, so at least we can invite someone to join. That&#x27;s what I&#x27;ve been thinking about regarding authentication.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;family-time-and-coffee&quot;&gt;Family Time and Coffee&lt;&#x2F;h2&gt;
&lt;p&gt;The rest of the day, from 2 PM to probably 8 or 9 PM, I spent with the kid. We went to a social club that had an event with a company that brings animals for interaction. It was actually better than I expected. We had a bunch of friendly animals the kids could play with, touch, and pet. For some reason, I also enjoyed it; I&#x27;m not sure why, but it was a good break.&lt;&#x2F;p&gt;
&lt;p&gt;This morning, I had a coffee from Ground Hill Roasters. It&#x27;s a very good coffee, I believe it&#x27;s a Colombian. I&#x27;d have to check. It&#x27;s also a fermented coffee, which I guess is the thing. I brewed it for espresso, and what really impressed me was that it truly kept its character in the espresso itself. It&#x27;s quite an acidic coffee, but good. I haven&#x27;t dialed it in perfectly yet; I need to grind it a bit finer, but it was a very interesting coffee. The notes, I think, mention something like &quot;winey&quot; on the front. I&#x27;m not sure if I got the winey part itself, but it did have a bit of a reminiscent of an alcoholic beverage. Yeah, very fruity, like red fruits, sort of like an overripe blackberry from the bush, almost fermenting on the bushâ€”that&#x27;s the feeling I got from it. Very nice.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;garden-work-and-future-plans&quot;&gt;Garden Work and Future Plans&lt;&#x2F;h2&gt;
&lt;p&gt;Then we came home, and I basically cleaned the garden. We cut the grass on the lawn. I don&#x27;t have an electric mower; I have one of these reel mowers. It&#x27;s a very old design with a reel of blades. It&#x27;s really quiet. If you keep on top of a lawn and don&#x27;t let it go too wild, it&#x27;s so easy and actually quite fast. I&#x27;ve used other mowers before, and this is so much better. They are super quiet, the finish is super nice, and you can cut really close to the ground, so you have very short grass, which looks really good. I really, really like it. You have to pass it maybe once a week or so, and that&#x27;s fantastic. Also, there&#x27;s no gas, no electricity, no cables, and no batteries to charge. Just take it out, pass it, it&#x27;s great. I should do some maintenance on it though; I need to oil it, which I haven&#x27;t done yet, and I&#x27;ve been using it for months. They are the best, and they are also quite cheap. Sometimes, old technology is better.&lt;&#x2F;p&gt;
&lt;p&gt;I also have to start thinking what I&#x27;m going to plant. The tomatoes are starting to ripen now, so the raised bed will probably be free in a month. Then I also want to expand. I have some flower beds that I think I will use for greens now. I have to start thinking what I&#x27;m going to plant on them.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>MNT Reform Upgrades</title>
        <published>2025-08-01T00:00:00+00:00</published>
        <updated>2025-08-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-08-01/"/>
        <id>https://ruben.io/journal/2025-08-01/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-08-01/">&lt;p&gt;Today, I&#x27;ve been working on Epoch for Catacloud, specifically adding user metrics. Initially, I encountered an issue where handling a create command without previous state was problematic due to the segregated CRUD event structure. My temporary solution involved leveraging &quot;upsert&quot; for state persistence, treating every metric update as a creation event to allow &quot;upsert&quot; to handle modifications. While functional, this approach was a workaround. With the recent simplification of Epoch&#x27;s event and command handling, where CRUD segregation has been removed, a more elegant solution is now possible.&lt;&#x2F;p&gt;
&lt;p&gt;A significant challenge emerged when determining where to issue the event that updates these metrics. Initially, I considered adding it to the &lt;code&gt;handle delete&lt;&#x2F;code&gt; functions for files and file parts. However, this metrics change event isn&#x27;t part of the file aggregate; it belongs to the user aggregate. This realization highlighted the need for sagas, which weren&#x27;t yet implemented in Epoch. Without sagas, mixing file aggregate logic with user aggregate logic was unavoidable, which is not ideal. This problem was further compounded by the desire to support organizations in Catacloud, where metrics would need to be tracked at an organizational level, necessitating sagas to manage changes based on data store changes.&lt;&#x2F;p&gt;
&lt;p&gt;To address these fundamental architectural issues, I&#x27;ve made substantial progress on Epoch. I&#x27;ve simplified event and command handling, moving towards essentially only commands and events, removing the problematic CRUD segregation. Handlers now receive only the events they are interested in through a subset enum for filtering, which has significantly improved the workflow. I also added a new Saga trait, which I am currently testing, directly enabling the management of cross-aggregate concerns like organizational metrics. This simplification in event handling, however, requires substantial changes in Catacloud, as a single, larger handler is now needed for all events of every Aggregate.&lt;&#x2F;p&gt;
&lt;p&gt;Furthermore, I&#x27;ve added robust error handling to Epoch. Previously, everything was &quot;boxed,&quot; but now we have specific errors for system occurrences, which can be mapped to precise error responses. This is a major improvement. As expected, given these changes, Catacloud currently shows many errors, and I will continue working on this tomorrow.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mnt-reform-upgrades&quot;&gt;MNT Reform Upgrades&lt;&#x2F;h2&gt;
&lt;p&gt;I recently received an antenna and pigtail for to upgrade my MNT Reform. I installed it by drilling a 6mm hole in the side plate to pass the connector through and then connected the pigtail to the Wi-Fi auxiliary port. The installation looks good, almost as if it came with the device. My only concern is that when I rotate the connector to tighten it, the pigtail sometimes rotates, which could damage it. I need to find a way to lock it in place to prevent rotation.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ve conducted some tests, and without the antenna, the coverage is similar to before. With the antenna, I&#x27;ve seen a considerable boost, about 10-15% more coverage reported by the network manager. Having an external antenna is clearly beneficial, even though this one is low gain (3 dbi). I could potentially use a larger, omnidirectional antenna for even more boost. I&#x27;m even considering a larger car antenna for experiments, like connecting to networks or capturing packages, reminiscent of what I did as a kid. I&#x27;m also interested in experimenting with an SDR system integrated into the Reform, given its internal space.&lt;&#x2F;p&gt;
&lt;p&gt;On the cosmetic side, I&#x27;d like to experiment with changing the backplate of the screen to integrate an embedded antenna, which would further increase coverage. The only hesitation is that I really like the current screen backplate and don&#x27;t want to replace it with a PLA or ABS one, though I might try to see how it looks.&lt;&#x2F;p&gt;
&lt;p&gt;Another recent development is receiving a new cable for my headphones. The old one broke, and I couldn&#x27;t fix it. The new cable has a larger, screwing jack (not 3.5mm, maybe 6mm) with an adapter. The MNT Reform&#x27;s jack connector is recessed, and the threads on the new jack prevented a proper fit. I had to file down the threads to make it connect. It looks okay; the filed finish almost adds to the industrial aesthetic of the cable, and it connects fine to the Reform, which was my main goal. This allows for proper audio.&lt;&#x2F;p&gt;
&lt;p&gt;For future audio upgrades, I want to address the lack of an internal microphone on the MNT Reform. I need a microphone for recordings and video calls. Currently, I use a USB microphone, which works well, but I&#x27;d prefer to use the Reform&#x27;s audio input, which I believe supports both audio in and out. I might get new headphones with an integrated microphone. While the audio quality might not be as good as the USB microphone, it offers portability without carrying a large microphone.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ve also ordered a basic webcam, which hasn&#x27;t arrived yet. In the future, I might experiment with building my own webcam using a Raspberry Pi Zero for better image quality and more openness than commercial webcams. This setup would allow me to connect the webcam only when needed and potentially install it on my desk, connecting via USB.&lt;&#x2F;p&gt;
&lt;p&gt;Other potential upgrades for my MNT Reform include adding a serial port connector and possibly an internal FPGA (like the tiny FPGA Bx) for hacking. While I can always plug in an FPGA via USB, having it integrated would be convenient for on-the-go hacking. All these USB-based additions necessitate an internal USB hub. The Reform&#x27;s motherboard has a couple of connectors, one for the trackball and another for the keyboard. I could disconnect one, add a hub, and gain more internal USB ports for devices like a Bluetooth receiver.&lt;&#x2F;p&gt;
&lt;p&gt;The concern with an internal USB hub is increased energy consumption. I need to explore options for switching off unused USB devices, either with a physical switch or through software. For the USB hub itself, I might get an off-the-shelf one or even design my own board based on the open hardware Nano Hub design, modifying it to fit the Reform&#x27;s connectors without soldering. This would be a good learning experience.&lt;&#x2F;p&gt;
&lt;p&gt;Overall, I&#x27;m really enjoying this laptop. It&#x27;s become my main computer, and I&#x27;ve been using it for a few weeks without touching my desktop. It&#x27;s great.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Epoch Challenges</title>
        <published>2025-07-31T00:00:00+00:00</published>
        <updated>2025-07-31T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-07-31/"/>
        <id>https://ruben.io/journal/2025-07-31/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-07-31/">&lt;p&gt;Today, I focused significantly on Epoch, but also dedicated time to the GFF3 implementation. I updated the documentation for the GFF3 validation rules, incorporating new code examples and a diagram, and removed outdated dependencies. I believe the documentation is now in a very good state.&lt;&#x2F;p&gt;
&lt;p&gt;I also identified and resolved a small bug in the rule state manager and we closed the Pull Request that added validation for duplicated sequences. I need to spend more time considering the validation engine, and the new encoding and decoding processes for sequences.&lt;&#x2F;p&gt;
&lt;p&gt;I received confirmation that there&#x27;s no masking on the ENA sequences. This is excellent news because it means I should be able to utilize three bits per base for encoding, which, based on some tests I&#x27;ve done, will lead to a substantial reduction in storage needs, this could result in approximately a 75% reduction in size after compression. This is quite exciting, and I&#x27;m surprised no one has implemented this before, though perhaps I&#x27;m overlooking something.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;aggregate-implementation-bug&quot;&gt;Aggregate Implementation Bug&lt;&#x2F;h3&gt;
&lt;p&gt;I encountered multiple limitations in Epoch today. The first issue was that my aggregate implementation was saving events &lt;em&gt;before&lt;&#x2F;em&gt; they were persisted in the aggregate store. When an event is saved, it propagates, meaning projections were being called before the aggregate&#x27;s state was persisted. This led to situations where a projection might require the aggregate&#x27;s state, but it didn&#x27;t yet exist, resulting in errors (not null pointers, as I&#x27;m using options, but similar state-not-found errors).&lt;&#x2F;p&gt;
&lt;p&gt;This is clearly not ideal. It feels as though events should be emitted &lt;em&gt;after&lt;&#x2F;em&gt; both the state and the events are persisted. Currently, it&#x27;s all tangled together in the event store, which is problematic. As a quick fix, I&#x27;m now saving events &lt;em&gt;after&lt;&#x2F;em&gt; the state. However, this isn&#x27;t a long-term solution. I don&#x27;t even think transactions would fully resolve it, as the same issue could arise if projections are not within the same system, leading to latency-dependent bugs â€“ the worst kind to find.&lt;&#x2F;p&gt;
&lt;p&gt;I have an intuition for a proper resolution: separating emission from persistence. The persistence would be done in a transaction, and the emission would happen afterwards. This seems to be the best way forward. The state for both events and aggregate state would be persisted in an atomic operation. We could then have an index of emitted events, allowing for re-emission later if, for example, the event bus service is down. From an eventual consistency perspective, this would be acceptable because the source of truth (events and aggregate state) would be persisted.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;time-series-creation-limitation&quot;&gt;Time Series Creation Limitation&lt;&#x2F;h3&gt;
&lt;p&gt;I found another limitation in Epoch while trying to create a time series from a couple of events using a projection. Specifically, I needed to create a time series from &lt;code&gt;file part uploaded&lt;&#x2F;code&gt; and &lt;code&gt;file deleted&lt;&#x2F;code&gt; events to track user storage usage over time. This required knowing the last event in the time series to calculate the new event based on it.&lt;&#x2F;p&gt;
&lt;p&gt;My initial approach was constrained by Epoch&#x27;s current implementation. To resolve this, I decided to add a new event called &lt;code&gt;UserFileStorageChanged&lt;&#x2F;code&gt;. This event will be emitted in both &lt;code&gt;file part uploaded&lt;&#x2F;code&gt; and &lt;code&gt;file deleted&lt;&#x2F;code&gt; scenarios, and will include the total storage used plus the delta between the old state and the new state.&lt;&#x2F;p&gt;
&lt;p&gt;I also introduced two new projections:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;One to store this &lt;code&gt;UserFileStorageChanged&lt;&#x2F;code&gt; event directly into a time series.&lt;&#x2F;li&gt;
&lt;li&gt;Another, &lt;code&gt;UserMetrics&lt;&#x2F;code&gt;, to calculate the current user storage usage. I can then query this projection from the aggregate for files and use it to calculate the total storage used for the new event.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;This new approach allows me to calculate both the current user storage usage and show historical changes, which is quite useful. Interestingly, this limitation actually led me to a better solution than my initial intent. However, I still believe it highlights a limitation in Epoch that I would like to address in the future.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m starting to believe that the &lt;code&gt;create&lt;&#x2F;code&gt;, &lt;code&gt;update&lt;&#x2F;code&gt;, and &lt;code&gt;delete&lt;&#x2F;code&gt; constraints I have are too restrictive. The &lt;code&gt;enum&lt;&#x2F;code&gt; subset logic I have implemented to filter events is good, but limiting it to just these crud event categories is short-sighted. There&#x27;s much more to it. Also, on the store side, when retrieving state, it should be possible to get events and then decide how to retrieve the state based on those events. This would allow for potential joins or picking state from multiple tables, as currently it&#x27;s limited to a single table, which is also not ideal. The persistence side is similar. I need to generalize it more to remove many of these limitations, but for now, I&#x27;ll continue with the current approach.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;metrics-projection-creation-limitation&quot;&gt;Metrics Projection Creation Limitation&lt;&#x2F;h3&gt;
&lt;p&gt;Adding to the complexities, the new &lt;code&gt;UserMetrics&lt;&#x2F;code&gt; projection currently has no creation event. Its creation would logically align with user creation, but I don&#x27;t currently have an aggregate for the user. This means that for metrics, there are only update events, and no creation event.&lt;&#x2F;p&gt;
&lt;p&gt;Epoch currently limits updates to always have an initialized state; if there&#x27;s no state in the database, it will fail. This is a significant shortcoming. I&#x27;m unsure how to work around this. My options are to either add a projection for the user (which I&#x27;m not keen on) or modify Epoch. I don&#x27;t see another immediate solution right now. It&#x27;s late, so I&#x27;ll drop this for today. It&#x27;s been a very long day.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>GFF3 Implementation: Validation and Transformation</title>
        <published>2025-07-30T00:00:00+00:00</published>
        <updated>2025-07-30T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-07-30/"/>
        <id>https://ruben.io/journal/2025-07-30/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-07-30/">&lt;p&gt;Today, I continued working on the EBI GFF3 implementation, focusing on adding validations for out-of-order entries. This is crucial when transforming annotations from GFF3 to EMBL or other flat files, as the EMBL format doesn&#x27;t support multiple entries with the same sequence identifier.&lt;&#x2F;p&gt;
&lt;p&gt;In GFF3, our streaming implementation allows processing files without loading the entire file into memory. The GFF3 resolution directive (###) can define that all previous entries have no dependencies, meaning these entries can be validated in isolation. However, this doesn&#x27;t prevent entries further down the file from belonging to the same sequence ID.&lt;&#x2F;p&gt;
&lt;p&gt;When transforming GFF3 to EMBL, if such a directive appears, we might create an annotation and then continue with the same sequence ID, issuing it as a new annotation. This would result in a new EMBL entry with the same sequence ID, which is not allowed by the ENA databases.&lt;&#x2F;p&gt;
&lt;p&gt;To address this, we currently merge annotations if the previous one has the same sequence ID. However, a challenge arises if sequence ID 1 appears, followed by sequence ID 2, and then sequence ID 1 again. This indicates that features in the GFF3 file might not be completely in order, which is valid according to the spec but problematic for EMBL transformation.&lt;&#x2F;p&gt;
&lt;p&gt;My recent work involved adding an annotation to address this. A new rule, enforceable via the previously developed rule system, allows filtering or validating GFF3 files during completion, preventing these issues.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;new-validation-engine-granularity-and-semantic-validations&quot;&gt;New Validation Engine: Granularity and Semantic Validations&lt;&#x2F;h2&gt;
&lt;p&gt;I&#x27;ve also begun working on a new validation engine, currently in the definition phase. This engine provides an abstraction on top of existing rule sets, enabling validations at different levels of granularity.&lt;&#x2F;p&gt;
&lt;p&gt;For example, validations can now operate at the line level, requiring multiple lines, or even an entire file. We&#x27;re achieving this by accumulating state internally within the validations as we read the file and call them with annotations. This allows validations to build graphs or other data structures for recognition and validation during the streaming process.&lt;&#x2F;p&gt;
&lt;p&gt;This is particularly important as we&#x27;ll be adding semantic validations that consider biological features, which often depend on other features within the file. Currently, our validations are purely syntactic, ensuring the GFF3 file is valid and convertible to an EMBL file. The new process will expand this, with these new validations regulated by rule sets, allowing them to be enabled or disabled, or even soft-disabled (issuing a warning without halting the process). This is currently a theoretical focus, with implementation to follow soon.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;epoch-to-catacloud-integration-aggregations-and-projections-for-files&quot;&gt;Epoch to Catacloud Integration: Aggregations and Projections for Files&lt;&#x2F;h2&gt;
&lt;p&gt;In the evening, I&#x27;ll be working on adding Epoch to Catacloud, specifically focusing on aggregations and projections for files. In Catacloud, we have a concept of files, file uploads, and file parts. This is because we handle large point cloud files (multiple gigabytes) and want to allow users to upload them in chunks, re-uploading only failed parts.&lt;&#x2F;p&gt;
&lt;p&gt;My implementation involves a file aggregate, with file uploads and file parts as projections of that aggregate. The aggregate dictates the logic for persisting the file, currently using an S3 abstraction (though not directly S3). It propagates events to maintain the state of files and parts being uploaded, tracking missing parts to enable partial retries and resume of file uploads.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ve refactored much of this, but haven&#x27;t tested it yet, and the seed is currently failing. The seed previously hardcoded files for testing, but now files need to be generated dynamically. This presents a challenge, as I was directly pushing files to Minio and creating entities in the database. Now, I need to create an event that initiates an upload, and then another event to put the file in Minio.&lt;&#x2F;p&gt;
&lt;p&gt;This event-driven approach can be a bit involved, making state representation for testing more complex. While I could hardcode the state, it wouldn&#x27;t be representative for tests. The goal of the seed is to populate the database with meaningful data for testing.&lt;&#x2F;p&gt;
&lt;p&gt;If this goes well, I anticipate completing this tomorrow and then focusing more on projections for billing.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Tackling Asynchronous Challenges</title>
        <published>2025-07-29T00:00:00+00:00</published>
        <updated>2025-07-29T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-07-29/"/>
        <id>https://ruben.io/journal/2025-07-29/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-07-29/">&lt;p&gt;Today, my primary focus was on the integration of Epoch and Catacloud, specifically around adding events. This work mainly took place in the afternoon and evening, as the morning and part of the afternoon were dedicated to childcare. I ended up working until 3 AM.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;file-and-job-linking-in-epoc&quot;&gt;File and Job Linking in Epoc&lt;&#x2F;h3&gt;
&lt;p&gt;The integration with Epoch and Catacloud revolved around linking files and jobs. I implemented two types of events: &lt;code&gt;link file&lt;&#x2F;code&gt; and &lt;code&gt;unlink file&lt;&#x2F;code&gt;. When files are removed from a job, an &lt;code&gt;unlink&lt;&#x2F;code&gt; command is issued, and when files are added, a &lt;code&gt;link&lt;&#x2F;code&gt; command is issued.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;addressing-asynchronous-event-propagation&quot;&gt;Addressing Asynchronous Event Propagation&lt;&#x2F;h3&gt;
&lt;p&gt;I encountered an asynchronous problem where the &lt;code&gt;job file link&lt;&#x2F;code&gt; (or &lt;code&gt;job file&lt;&#x2F;code&gt; projection) was not immediately updated after events were sent to the job aggregate. This meant the preview displayed old files, which was frustrating. To resolve this temporarily, I switched the event bus from the Postgres event bus to an in-memory event bus. This ensures instantaneous propagation, projecting the changes as soon as the event is emitted, without the delay of going through Postgres.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;lessons-from-iris-local-handlers-for-event-buses&quot;&gt;Lessons from Iris: Local Handlers for Event Buses&lt;&#x2F;h3&gt;
&lt;p&gt;This issue reminded me of a similar problem I solved with Iris, a library I developed a few years ago for event-driven systems (prior to adopting event sourcing). In Iris, if the event emitter and subscriber were in the same process, the handler would be called directly in addition to emitting the event to the bus for other subscribers. This significantly reduced latency. I believe a similar functionality will need to be added to the Epoch Postgres event bus, and potentially to future Kafka or RabbitMQ event buses, to reduce latency for local subscribers. While this isn&#x27;t ideal, as files attached to a job feel like part of the job projection, it resolves the immediate problem.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;enhancing-epoch-s-persistence-layer&quot;&gt;Enhancing Epoch&#x27;s Persistence Layer&lt;&#x2F;h3&gt;
&lt;p&gt;Another area for improvement in Epoch is its persistence layer. Currently, the &lt;code&gt;get&lt;&#x2F;code&gt; method has a hard-coded query, and the &lt;code&gt;persist&lt;&#x2F;code&gt; method only handles single entity saves. Ideally, the &lt;code&gt;persist&lt;&#x2F;code&gt; interface should allow saving multiple entities simultaneously. For instance, in our scenario, we have a &lt;code&gt;job&lt;&#x2F;code&gt; table and a &lt;code&gt;job_file&lt;&#x2F;code&gt; intermediary table. The &lt;code&gt;job&lt;&#x2F;code&gt; aggregate should be able to persist both the &lt;code&gt;job&lt;&#x2F;code&gt; entity and multiple &lt;code&gt;job_file&lt;&#x2F;code&gt; entities at the same time, logically representing a job with an array of files. This improvement to the Epoch library is necessary to better support these types of projections.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;progress-and-future-file-projections-and-billing&quot;&gt;Progress and Future: File Projections and Billing&lt;&#x2F;h3&gt;
&lt;p&gt;Despite these challenges, file linking is now working, and I&#x27;ve resolved several other related issues in both the UI and backend. I&#x27;ve also started migrating files to projections. This is a crucial step that will enable me to measure the volume of storage a specific user is consuming, which will be used for billing purposes (e.g., gigabytes per hour). My next steps involve adding an aggregate for this and then a projection to calculate &quot;gigabytes per unit of time&quot; as the basis for charging.&lt;&#x2F;p&gt;
&lt;p&gt;This was a productive day, and I&#x27;m making steady progress towards the billing infrastructure.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Epoch Integration, Projection Puzzles, and Billing Blueprints.</title>
        <published>2025-07-28T00:00:00+00:00</published>
        <updated>2025-07-28T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-07-28/"/>
        <id>https://ruben.io/journal/2025-07-28/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-07-28/">&lt;p&gt;Today, I primarily focused on the Catacloud and Epoch integration, which progressed well. The only adjustment I had to do to Epoch was to include the aggregate ID in commands, as it was missing.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;epoch-error-handling-and-aggregate-design&quot;&gt;Epoch Error Handling and Aggregate Design&lt;&#x2F;h3&gt;
&lt;p&gt;I&#x27;ve realized that Epoch&#x27;s error handling needs significant improvement. The current approach of ubiquitous &lt;code&gt;Box&lt;&#x2F;code&gt; for errors makes handling cumbersome and downcasting annoying. This is a critical area for future work, though the current implementation is minimally viable for my immediate needs.&lt;&#x2F;p&gt;
&lt;p&gt;I successfully implemented the &lt;code&gt;Job&lt;&#x2F;code&gt; aggregate and refined the db seeds to generate correct entities using events. This aggregate and events allows me to calculate the run duration for a user over an interval, providing total runtime in seconds across their jobs. I also added the specific job&#x27;s runtime into the UI, calculated from its start and completion times, which will serve as the foundation for billing cpu usage.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;job-files-and-projection-challenges&quot;&gt;Job Files and Projection Challenges&lt;&#x2F;h3&gt;
&lt;p&gt;A significant challenge emerged with job files, specifically their linking and unlinking. Job files are managed through an intermediary table, allowing file reuse across multiple jobs. My current system applies a command to a job, triggering link and unlink events. The UI unlinks all files from a job before relinking only the selected ones.&lt;&#x2F;p&gt;
&lt;p&gt;The core issue lies in a new projection for job files. This projection reacts to events from the job aggregate to hydrate file link states. However, the &quot;unlink files&quot; command is generic, unlinking all files for a given job ID. My current CRUD system, designed for single-entity operations (create, update, delete), doesn&#x27;t elegantly handle events affecting multiple entities, like batch deletions or updates. This makes managing projections tied to specific IDs tricky when a generic command affects a collection of files.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m currently storing this state in Postgres and contemplating using the job ID as the identifier for the file state, essentially treating the state as a list of file IDs. While this feels a bit hacky and might impact performance, it&#x27;s a direction I&#x27;m exploring. I need to devise a more flexible Epoch model to accommodate events that create, update, or delete multiple entities without forcing users to implement specialized multi-entity methods.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;future-work-storage-billing&quot;&gt;Future Work: Storage Billing&lt;&#x2F;h3&gt;
&lt;p&gt;Next, I need to finalize the job files implementation. Subsequently, I&#x27;ll focus on calculating &quot;gigabytes per hour&quot; or &quot;gigabytes-hours&quot; for file storage. This metric will be crucial for billing users for storage consumption. The challenge here is how to attribute storage cost for input files shared across multiple jobs. The cost might need to be divided among jobs that utilize the same file, or perhaps only consider files not shared by other users.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h3&gt;
&lt;p&gt;Overall, it was a productive day with some significant progress and a clear understanding of immediate and future challenges. I&#x27;m quite tired but remain focused on the &quot;money part&quot; of the project, which is vital for its completion. There&#x27;s still a lot to do.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Epoch Integration, USB Woes, and Foraged Tea</title>
        <published>2025-07-27T00:00:00+00:00</published>
        <updated>2025-07-27T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-07-27/"/>
        <id>https://ruben.io/journal/2025-07-27/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-07-27/">&lt;p&gt;Today was primarily dedicated to integrating Epoch into Catacloud. As with most integration efforts, I encountered several unexpected discoveries.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;epoch-cloud-integration-details&quot;&gt;Epoch Cloud Integration Details&lt;&#x2F;h3&gt;
&lt;p&gt;A significant part of the day involved implementing a new PostgreSQL-backed state store for persistent aggregate states, as I mentioned yesterday. This was quite straightforward, though I haven&#x27;t yet added comprehensive transaction support. My goal is to ensure consistency across different traits, which will require further work. For now, I focused on getting a functional, albeit imperfect, solution in place.&lt;&#x2F;p&gt;
&lt;p&gt;I also introduced a command wrapper, analogous to our existing event wrappers. This wrapper encapsulates command data, optional credentials, and the expected aggregate version within a command struct. While I&#x27;m still evaluating its full utility given that aggregates manage diverse command types, the consistent struct with varying command data and optional credentials seems promising.&lt;&#x2F;p&gt;
&lt;p&gt;A major structural change involved re-conceptualizing aggregates as a specialized type of projection. My research, heavily emphasized separating projections (optimized for read-side operations) from aggregates (optimized for write-side operations). However, aggregates necessitate snapshots, which inherently function as a form of projection.&lt;&#x2F;p&gt;
&lt;p&gt;My initial approach, using distinct Projection and Aggregate traits, led to duplicated snapshot creation logic within the aggregate. To address this, my current approach separates event generation from projection. The aggregate trait now focuses solely on handling commands, enforcing business logic (determining if a command should proceed or return an error), and emitting events. It then offloads to the projection side, which takes these events, hydrates them onto the existing state, and returns the updated state to the aggregate for persistence alongside the events. This effectively reuses projection logic within the aggregate, making the aggregate a specialized projection. While this might raise an eyebrow or two among Event Sourcing purists, it maintains a clear separation of concerns: aggregates manage business logic and event emission, while projections handle state. I believe this solution is both effective and elegant, and it naturally allows for other types of projections that are not aggregates. I&#x27;m currently implementing this in Catacloud, and it appears to be working well, though some minor refinements may still be needed. This architectural shift consumed the majority of my day.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;mnt-reform-usb-issue&quot;&gt;MNT Reform USB Issue&lt;&#x2F;h3&gt;
&lt;p&gt;On a less technical note, I unfortunately broke one of the USB ports on my MNT Reform. I&#x27;m unsure how it happened; I simply pulled out the microphone, and a piece of plastic from the USB port detached. I later found this plastic piece inside the microphone&#x27;s USB connector. The port remains functional, but it&#x27;s now missing the structural support for the USB pins. I&#x27;ll need to add a dust cover or similar protection to prevent further damage. If I manage to find the plastic piece again, I might attempt to re-glue it. It was a peculiar incident, but thankfully, it&#x27;s not a critical issue.&lt;&#x2F;p&gt;
&lt;p&gt;Despite this minor setback, I remain very satisfied with my computer. Its performance and feel are excellent, fulfilling all my expectations. I highly recommend it.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;coffee-and-tea-reflections&quot;&gt;Coffee and Tea Reflections&lt;&#x2F;h3&gt;
&lt;p&gt;Beyond the main project work, I don&#x27;t recall any other significant tasks today. However, I must mention a delightful coffee I had yesterday but didn&#x27;t report. It&#x27;s a truly excellent coffee from Majorelle, Napo, Ecuador. The tasting notes mention &quot;Elderberry, persimmon, and lilac.&quot; I particularly enjoyed its distinct fermented aroma. It&#x27;s a washed coffee, labeled &quot;Washed Sidra.&quot; I&#x27;m not entirely sure what &quot;Sidra&quot; signifies in the processing, but it certainly carries the aromatic qualities of a naturally processed coffee. It&#x27;s a very intriguing coffee, though I&#x27;m not entirely convinced by the proposed notes; I detected a hint of lavender and a slightly soapy finish. Nevertheless, it&#x27;s a very pleasant coffee, characteristic of the quality I&#x27;ve come to expect from Sweven coffees, albeit quite expensive.&lt;&#x2F;p&gt;
&lt;p&gt;Currently, I&#x27;m enjoying a cup of tea made from mugwort flowers I foraged a few days ago during a walk on the Roman Road. I collected a good amount, dried them, and now brew them for tea. It&#x27;s lovely, and my daughter enjoys it too.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;future-plans&quot;&gt;Future Plans&lt;&#x2F;h3&gt;
&lt;p&gt;That pretty much summarizes my day. I anticipate writing a comprehensive blog post once the initial Epoch implementation is complete. This post will delve into the architectural decisions made and the rationale behind them, which I believe will make for an interesting read. That&#x27;s all for today, I think.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Hardware Upgrades, Coding Challenges, and UXN Insights</title>
        <published>2025-07-26T00:00:00+00:00</published>
        <updated>2025-07-26T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-07-26/"/>
        <id>https://ruben.io/journal/2025-07-26/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-07-26/">&lt;p&gt;Today brought some exciting developments, particularly concerning my MNT Reform. I received new, higher-capacity batteries (3200 mAh), which is fantastic. More importantly, these new batteries have resolved my persistent battery balance problem. I no longer have one battery overcharged while the others are drained, ensuring my computer won&#x27;t unexpectedly shut down again. While I&#x27;m still awaiting the charger, the batteries seem to be working well, and I plan to balance them properly once it arrives next week.&lt;&#x2F;p&gt;
&lt;p&gt;My work on the Epoch library continued today. I successfully added an example using the aggregator, and it appears to be functioning as expected within the example environment. I then began the process of integrating it into Catacloud, which is its ultimate destination. However, this led me down a significant rabbit hole: I found myself back in Epoch, implementing transactions. I now realize this detour was due to my oversight in not having the Postgres state store implemented. My thought process was, &quot;I might as well add transactions now and do it directly with transactions.&quot; This, however, was a misstep. Implementing transactions for the in-memory version of Epoch, if done properly, would be a substantial amount of workâ€”far more than I initially anticipated. I&#x27;m considering dropping this specific transaction implementation for now and revisiting it at a later date.&lt;&#x2F;p&gt;
&lt;p&gt;The importance of transactions and atomic operations across multiple stores cannot be overstated. If I can implement transactions with both the event store (which could be Kafka) and the state store (perhaps Postgres or another solution), it would guarantee that events and state are saved atomically within the aggregate. Currently, my process involves first persisting events and then the state. This non-atomic approach creates a vulnerability: if state persistence fails, I have no way to roll back the already published events. This could lead to a situation where I have to rehydrate the aggregate or repeatedly attempt to save the state. If another operation then modifies that same state, it could result in a corrupted state. While the events themselves remain intact, allowing for state rehydration, it&#x27;s far from ideal. An atomic operation would provide much greater consistency and reliability.&lt;&#x2F;p&gt;
&lt;p&gt;Beyond coding, I&#x27;ve been delving deeper into the UXN virtual machine. I came across a fascinating blog post (https:&#x2F;&#x2F;bullno1.com&#x2F;blog&#x2F;type-checking-with-symbolic-execution), &lt;code&gt;bullno1&lt;&#x2F;code&gt; implemented a language server, a debugger, and other verification tooling for it. It was incredibly insightful, and I&#x27;m keen to explore it further. I believe I&#x27;ll need to implement something similar within my own game. The intriguing question is whether I can achieve this directly with UXN. If so, it would be a significant breakthrough. My goal is to implement a virtual machine that can run on my development machine and also be embedded within the game. This would allow me to reuse the same software across my entire development stack, significantly accelerating the development process, as I could develop and test on my machine and then seamlessly integrate it into the game without needing separate in-game tooling.&lt;&#x2F;p&gt;
&lt;p&gt;Overall, that pretty much sums up my day. Despite accomplishing several tasks, I don&#x27;t feel it was particularly productive, mainly because progress felt very slow. The event store work, in particular, has been a months-long endeavor, a very slow grind. I&#x27;m hopeful for a breakthrough soon so I can finally move past it.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>A Day of Tech Tinkering and Deep Dives</title>
        <published>2025-07-25T00:00:00+00:00</published>
        <updated>2025-07-25T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://ruben.io/journal/2025-07-25/"/>
        <id>https://ruben.io/journal/2025-07-25/</id>
        
        <content type="html" xml:base="https://ruben.io/journal/2025-07-25/">&lt;p&gt;Today was a bit of a whirlwind, starting with me waking up late â€“ still not entirely sure what happened there! Despite the slow start, I managed to dive into a few interesting projects.&lt;&#x2F;p&gt;
&lt;p&gt;My MNT reform&#x27;s Wi-Fi coverage has been bothering me, so I spent some time looking into antennas. I&#x27;ve got an external antenna coming, which means I&#x27;ll need to drill a hole in one of the side panels. The hope is that by combining an internal antenna with this new external one, I&#x27;ll finally get the coverage I need.&lt;&#x2F;p&gt;
&lt;p&gt;Then there&#x27;s the MNT reform&#x27;s unbalanced batteries. It&#x27;s been frustrating having to stay constantly connected because one battery is full while the others are empty. Because one of the batteries is full, the charge controller won&#x27;t charge the remaining ones. And when I disconnect power, the system to shuts down soon after. I&#x27;ve ordered a charger that should arrive early next week, which will hopefully allow me to charge all the bateries to the same level.&lt;&#x2F;p&gt;
&lt;p&gt;My thoughts also drifted to UXN, the VM. I&#x27;m starting to seriously consider using it as the embedded computer system within a programming-puzzle video game I&#x27;m conceptualizing. The idea is that players would program in UXN to solve in-game challenges. I was exploring different concepts, like changing the file system from a device-based one to a block system, perhaps even borrowing ideas from Forth. It was a bit of a rabbit hole, but an interesting one.&lt;&#x2F;p&gt;
&lt;p&gt;Work for EBI also took up a good chunk of my day. I pushed some fixes to a GFF3 implementation PR I had open. The main change was shifting how directives are handled: instead of a general list, annotations now have specific directives tied to them, with separate directives for the entire GFF3 file. It was a bigger change than I anticipated, requiring some refactoring of the directive&#x27;s location.&lt;&#x2F;p&gt;
&lt;p&gt;I also ran a test on optimizing FASTA file reads. My initial thought was to trim down the base representation to 2 bits per base, but then I remembered the character for unrecognized bases, which forces a 3-bit representation. Further research led me to &quot;masking,&quot; which I need to look into more, as it might push the bit count to 4. This would significantly reduce our anticipated 75% size reduction to a mere 50%, which isn&#x27;t as extraordinary, but still good given the terabytes of FASTA files we have at EBI. The streaming support is crucial for these massive files, especially when we need to transform them on the fly without loading the entire file into memory.&lt;&#x2F;p&gt;
&lt;p&gt;Finally, my afternoon was dedicated to Epoch, the event-sourcing library I&#x27;m building for Catallactical. I finally added the &lt;code&gt;aggregate&lt;&#x2F;code&gt; trait, and it seems to be working well! I also implemented support for optimistic concurrency and conflict resolution. Tomorrow, I&#x27;ll be writing an example using this new trait, and then I&#x27;ll start integrating it into the Catacloud system â€“ the main reason I&#x27;m developing Epoch.&lt;&#x2F;p&gt;
&lt;p&gt;All in all, a productive day filled with technical challenges and progress. Let&#x27;s see what tomorrow brings!&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
